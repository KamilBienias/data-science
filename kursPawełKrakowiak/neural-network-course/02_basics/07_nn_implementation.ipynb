{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_nn_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamilBienias/data-science/blob/main/kursPawe%C5%82Krakowiak/neural-network-course/02_basics/07_nn_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsUWcfin6ZlT"
      },
      "source": [
        "### Implementacja prostej sieci neuronowej\n",
        "\n",
        "##### Kroki:\n",
        "    1. Zainicjowanie parametrów sieci\n",
        "    2. Propagacja wprzód\n",
        "    3. Obliczenie błędu predykcji\n",
        "    4. Propagacja wsteczna (uczenie modelu)\n",
        "    5. Test działania modelu\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkDZb9Nt04rg"
      },
      "source": [
        "# ######################################################\n",
        "# Etap 5. Odcinek: Implementacja prostej sieci neuronowej.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# wektor wejściowy o wymiarze 1x2, czyli jedna próbka i dwie cechy.\n",
        "# Na przykład długość i waga produktu\n",
        "X = np.array([1.4, 0.7])\n",
        "# Na przykład to cena produktu\n",
        "y_true = np.array([1.8])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc5rkn_m1IjB"
      },
      "source": [
        "# n_x to liczba neuronów w warstwie wejściowej\n",
        "# n_h to liczba neuronów w warstwie ukrytej\n",
        "# n_y to liczba neuronów w warstwie wyjściowej\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    # generuje macierz wag W1\n",
        "    W1 = np.random.rand(n_h, n_x)\n",
        "    # generuje macierz wag W2\n",
        "    W2 = np.random.rand(n_h, n_y)\n",
        "    return W1, W2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43XdqDa1exJ"
      },
      "source": [
        "# ta funkcja nakarmi naszą sieć\n",
        "# oblicza y_pred jako X*W1*W2\n",
        "def forward_propagation(X, W1, W2):\n",
        "    # H1 to macierz pośrednia\n",
        "    H1 = np.dot(X, W1)\n",
        "    y_pred = np.dot(H1, W2)\n",
        "    return H1, y_pred"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbaAd2jt1v8t"
      },
      "source": [
        "# funkcja liczy błąd predykcji\n",
        "def calculate_error(y_pred, y_true):\n",
        "    return y_pred - y_true"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3QFBEn_14Lj"
      },
      "source": [
        "# zwraca tylko predykcję (bez H1)\n",
        "def predict(X, W1, W2):\n",
        "    _, y_pred = forward_propagation(X, W1, W2)\n",
        "    return y_pred[0]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rClKyNy02CnX"
      },
      "source": [
        "# ona pozwoli nauczyć model\n",
        "def backpropagation(X, W1, W2, learning_rate, iters=1000, precision=0.0000001):\n",
        "    # karmimy sieć danymi\n",
        "    # oblicza y_pred jako X*W1*W2\n",
        "    H1, y_pred = forward_propagation(X, W1, W2)\n",
        "    # straty podczas każdej iteracji\n",
        "    train_loss = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        # błąd predykcji\n",
        "        error = calculate_error(y_pred, y_true)\n",
        "        # od końca aktualizuje wagi\n",
        "        # najpierw aktualizuje macierz wag W2\n",
        "        W2 = W2 - learning_rate * error * H1.T\n",
        "        # potem aktualizuje macierz wag W1 \n",
        "        W1 = W1 - learning_rate * error * np.dot(X.T, W2.T)\n",
        "\n",
        "        # oblicza predykcję na nowych macierzach wag\n",
        "        y_pred = predict(X, W1, W2)\n",
        "        print(f'Iter #{i}: y_pred {y_pred}: loss: {abs(calculate_error(y_pred, y_true[0]))}')\n",
        "        # dorzuca stratę do listy train_loss\n",
        "        train_loss.append(abs(calculate_error(y_pred, y_true[0])))\n",
        "\n",
        "        if abs(error) < precision:\n",
        "            break\n",
        "\n",
        "    return W1, W2, train_loss"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phVhArP520f3"
      },
      "source": [
        "def build_model():\n",
        "    # 2 neurony wejściowe, 2 ukryte, 1 output\n",
        "    W1, W2 = initialize_parameters(2, 2, 1)\n",
        "    \n",
        "    # wrzucamy wagi do algorytmu propagacji wstecznej ze wskaźnikiem uczenia 0.01\n",
        "    W1, W2, train_loss = backpropagation(X, W1, W2, 0.01)\n",
        "\n",
        "    model = {'W1': W1, 'W2': W2, 'train_loss': train_loss}\n",
        "\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg7M4xDv3JFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c3eb90-9cd7-4147-fa2a-274b1a279d99"
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter #0: y_pred 0.47775082418068876: loss: 1.3222491758193113\n",
            "Iter #1: y_pred 0.5203197164995874: loss: 1.2796802835004126\n",
            "Iter #2: y_pred 0.5624603087798719: loss: 1.2375396912201282\n",
            "Iter #3: y_pred 0.6041630898849911: loss: 1.195836910115009\n",
            "Iter #4: y_pred 0.6454096471221384: loss: 1.1545903528778616\n",
            "Iter #5: y_pred 0.6861742668168541: loss: 1.113825733183146\n",
            "Iter #6: y_pred 0.7264254411736586: loss: 1.0735745588263415\n",
            "Iter #7: y_pred 0.7661272789328607: loss: 1.0338727210671395\n",
            "Iter #8: y_pred 0.8052408159236343: loss: 0.9947591840763658\n",
            "Iter #9: y_pred 0.8437252210745323: loss: 0.9562747789254677\n",
            "Iter #10: y_pred 0.8815388936200005: loss: 0.9184611063799996\n",
            "Iter #11: y_pred 0.9186404479839603: loss: 0.8813595520160398\n",
            "Iter #12: y_pred 0.9549895839833127: loss: 0.8450104160166874\n",
            "Iter #13: y_pred 0.9905478414374655: loss: 0.8094521585625345\n",
            "Iter #14: y_pred 1.0252792398682151: loss: 0.7747207601317849\n",
            "Iter #15: y_pred 1.0591508056150682: loss: 0.7408491943849318\n",
            "Iter #16: y_pred 1.0921329902786123: loss: 0.7078670097213877\n",
            "Iter #17: y_pred 1.1241999858610763: loss: 0.6758000141389238\n",
            "Iter #18: y_pred 1.1553299432399835: loss: 0.6446700567600165\n",
            "Iter #19: y_pred 1.185505101647381: loss: 0.6144948983526191\n",
            "Iter #20: y_pred 1.2147118376107076: loss: 0.5852881623892925\n",
            "Iter #21: y_pred 1.2429406423347056: loss: 0.5570593576652945\n",
            "Iter #22: y_pred 1.2701860367729518: loss: 0.5298139632270482\n",
            "Iter #23: y_pred 1.296446433669521: loss: 0.5035535663304791\n",
            "Iter #24: y_pred 1.3217239556708602: loss: 0.47827604432913984\n",
            "Iter #25: y_pred 1.3460242182455233: loss: 0.4539757817544767\n",
            "Iter #26: y_pred 1.3693560856383364: loss: 0.4306439143616636\n",
            "Iter #27: y_pred 1.3917314074600646: loss: 0.4082685925399354\n",
            "Iter #28: y_pred 1.4131647428070897: loss: 0.38683525719291034\n",
            "Iter #29: y_pred 1.4336730780491806: loss: 0.3663269219508194\n",
            "Iter #30: y_pred 1.4532755436450078: loss: 0.34672445635499227\n",
            "Iter #31: y_pred 1.4719931345688557: loss: 0.3280068654311443\n",
            "Iter #32: y_pred 1.489848438177975: loss: 0.31015156182202497\n",
            "Iter #33: y_pred 1.5068653726340864: loss: 0.29313462736591367\n",
            "Iter #34: y_pred 1.5230689383266065: loss: 0.27693106167339354\n",
            "Iter #35: y_pred 1.538484984137377: loss: 0.26151501586262293\n",
            "Iter #36: y_pred 1.5531399898420248: loss: 0.2468600101579752\n",
            "Iter #37: y_pred 1.5670608654636993: loss: 0.23293913453630077\n",
            "Iter #38: y_pred 1.5802747679806675: loss: 0.21972523201933258\n",
            "Iter #39: y_pred 1.5928089354380999: loss: 0.20719106456190017\n",
            "Iter #40: y_pred 1.6046905382229746: loss: 0.19530946177702546\n",
            "Iter #41: y_pred 1.6159465470249987: loss: 0.1840534529750013\n",
            "Iter #42: y_pred 1.6266036168207825: loss: 0.17339638317921757\n",
            "Iter #43: y_pred 1.6366879860778398: loss: 0.16331201392216022\n",
            "Iter #44: y_pred 1.6462253902739443: loss: 0.15377460972605572\n",
            "Iter #45: y_pred 1.6552409887604627: loss: 0.14475901123953738\n",
            "Iter #46: y_pred 1.6637593039605376: loss: 0.13624069603946243\n",
            "Iter #47: y_pred 1.6718041718794512: loss: 0.12819582812054886\n",
            "Iter #48: y_pred 1.6793987029108668: loss: 0.12060129708913325\n",
            "Iter #49: y_pred 1.686565251944918: loss: 0.11343474805508214\n",
            "Iter #50: y_pred 1.6933253968187494: loss: 0.10667460318125066\n",
            "Iter #51: y_pred 1.699699924194076: loss: 0.10030007580592404\n",
            "Iter #52: y_pred 1.7057088219969534: loss: 0.09429117800304665\n",
            "Iter #53: y_pred 1.7113712776099892: loss: 0.08862872239001085\n",
            "Iter #54: y_pred 1.7167056810648456: loss: 0.08329431893515449\n",
            "Iter #55: y_pred 1.7217296325414804: loss: 0.07827036745851967\n",
            "Iter #56: y_pred 1.7264599535389993: loss: 0.07354004646100076\n",
            "Iter #57: y_pred 1.7309127011401988: loss: 0.06908729885980125\n",
            "Iter #58: y_pred 1.7351031848471579: loss: 0.06489681515284218\n",
            "Iter #59: y_pred 1.7390459855180396: loss: 0.06095401448196047\n",
            "Iter #60: y_pred 1.7427549759851646: loss: 0.057245024014835444\n",
            "Iter #61: y_pred 1.7462433429812245: loss: 0.05375665701877552\n",
            "Iter #62: y_pred 1.7495236100440095: loss: 0.050476389955990575\n",
            "Iter #63: y_pred 1.7526076611102195: loss: 0.04739233888978056\n",
            "Iter #64: y_pred 1.7555067645458284: loss: 0.04449323545417161\n",
            "Iter #65: y_pred 1.7582315973941325: loss: 0.04176840260586756\n",
            "Iter #66: y_pred 1.7607922696531624: loss: 0.039207730346837666\n",
            "Iter #67: y_pred 1.7631983484217237: loss: 0.036801651578276307\n",
            "Iter #68: y_pred 1.7654588817781245: loss: 0.034541118221875555\n",
            "Iter #69: y_pred 1.7675824222777847: loss: 0.03241757772221532\n",
            "Iter #70: y_pred 1.769577049975673: loss: 0.030422950024326934\n",
            "Iter #71: y_pred 1.771450394896963: loss: 0.028549605103036946\n",
            "Iter #72: y_pred 1.7732096588947255: loss: 0.026790341105274562\n",
            "Iter #73: y_pred 1.774861636846979: loss: 0.025138363153021093\n",
            "Iter #74: y_pred 1.7764127371572316: loss: 0.023587262842768464\n",
            "Iter #75: y_pred 1.7778690015329075: loss: 0.022130998467092544\n",
            "Iter #76: y_pred 1.7792361240248986: loss: 0.02076387597510143\n",
            "Iter #77: y_pred 1.7805194693191018: loss: 0.019480530680898278\n",
            "Iter #78: y_pred 1.7817240902772846: loss: 0.018275909722715422\n",
            "Iter #79: y_pred 1.7828547447301204: loss: 0.017145255269879645\n",
            "Iter #80: y_pred 1.7839159115298464: loss: 0.016084088470153635\n",
            "Iter #81: y_pred 1.7849118058738256: loss: 0.015088194126174459\n",
            "Iter #82: y_pred 1.785846393913453: loss: 0.014153606086547033\n",
            "Iter #83: y_pred 1.7867234066653805: loss: 0.013276593334619502\n",
            "Iter #84: y_pred 1.7875463532440663: loss: 0.012453646755933745\n",
            "Iter #85: y_pred 1.7883185334362184: loss: 0.011681466563781662\n",
            "Iter #86: y_pred 1.7890430496388667: loss: 0.01095695036113331\n",
            "Iter #87: y_pred 1.7897228181836415: loss: 0.010277181816358505\n",
            "Iter #88: y_pred 1.7903605800703621: loss: 0.009639419929637905\n",
            "Iter #89: y_pred 1.79095891113334: loss: 0.009041088866660107\n",
            "Iter #90: y_pred 1.791520231663874: loss: 0.008479768336125959\n",
            "Iter #91: y_pred 1.7920468155123246: loss: 0.007953184487675458\n",
            "Iter #92: y_pred 1.7925407986929023: loss: 0.0074592013070977625\n",
            "Iter #93: y_pred 1.7930041875139484: loss: 0.0069958124860516335\n",
            "Iter #94: y_pred 1.79343886625601: loss: 0.006561133743990144\n",
            "Iter #95: y_pred 1.7938466044194783: loss: 0.006153395580521748\n",
            "Iter #96: y_pred 1.7942290635629397: loss: 0.005770936437060392\n",
            "Iter #97: y_pred 1.7945878037527445: loss: 0.00541219624725553\n",
            "Iter #98: y_pred 1.7949242896435964: loss: 0.00507571035640364\n",
            "Iter #99: y_pred 1.7952398962092577: loss: 0.0047601037907423205\n",
            "Iter #100: y_pred 1.7955359141417324: loss: 0.00446408585826763\n",
            "Iter #101: y_pred 1.7958135549365493: loss: 0.004186445063450783\n",
            "Iter #102: y_pred 1.7960739556810235: loss: 0.003926044318976585\n",
            "Iter #103: y_pred 1.7963181835616495: loss: 0.003681816438350527\n",
            "Iter #104: y_pred 1.7965472401060396: loss: 0.003452759893960433\n",
            "Iter #105: y_pred 1.7967620651741159: loss: 0.0032379348258841922\n",
            "Iter #106: y_pred 1.7969635407125617: loss: 0.0030364592874383423\n",
            "Iter #107: y_pred 1.7971524942858599: loss: 0.002847505714140164\n",
            "Iter #108: y_pred 1.7973297023965797: loss: 0.002670297603420302\n",
            "Iter #109: y_pred 1.797495893606938: loss: 0.002504106393061978\n",
            "Iter #110: y_pred 1.7976517514730475: loss: 0.00234824852695259\n",
            "Iter #111: y_pred 1.7977979173026548: loss: 0.002202082697345231\n",
            "Iter #112: y_pred 1.7979349927466213: loss: 0.00206500725337877\n",
            "Iter #113: y_pred 1.7980635422338285: loss: 0.0019364577661715732\n",
            "Iter #114: y_pred 1.7981840952586885: loss: 0.001815904741311547\n",
            "Iter #115: y_pred 1.7982971485299186: loss: 0.0017028514700814235\n",
            "Iter #116: y_pred 1.7984031679887718: loss: 0.0015968320112282886\n",
            "Iter #117: y_pred 1.798502590704461: loss: 0.0014974092955390983\n",
            "Iter #118: y_pred 1.7985958266540698: loss: 0.0014041733459302375\n",
            "Iter #119: y_pred 1.7986832603938423: loss: 0.0013167396061577463\n",
            "Iter #120: y_pred 1.7987652526283424: loss: 0.0012347473716576296\n",
            "Iter #121: y_pred 1.7988421416836091: loss: 0.001157858316390925\n",
            "Iter #122: y_pred 1.798914244890077: loss: 0.0010857551099230367\n",
            "Iter #123: y_pred 1.7989818598806961: loss: 0.0010181401193039008\n",
            "Iter #124: y_pred 1.7990452658093747: loss: 0.000954734190625306\n",
            "Iter #125: y_pred 1.7991047244945615: loss: 0.000895275505438553\n",
            "Iter #126: y_pred 1.7991604814925055: loss: 0.0008395185074945299\n",
            "Iter #127: y_pred 1.7992127671044686: loss: 0.0007872328955313979\n",
            "Iter #128: y_pred 1.799261797321895: loss: 0.0007382026781050932\n",
            "Iter #129: y_pred 1.7993077747133348: loss: 0.0006922252866652379\n",
            "Iter #130: y_pred 1.799350889256658: loss: 0.0006491107433419518\n",
            "Iter #131: y_pred 1.7993913191199102: loss: 0.0006086808800898069\n",
            "Iter #132: y_pred 1.7994292313939486: loss: 0.0005707686060514305\n",
            "Iter #133: y_pred 1.7994647827798114: loss: 0.0005352172201886773\n",
            "Iter #134: y_pred 1.799498120233592: loss: 0.000501879766408031\n",
            "Iter #135: y_pred 1.7995293815714317: loss: 0.0004706184285683257\n",
            "Iter #136: y_pred 1.799558696037076: loss: 0.0004413039629240778\n",
            "Iter #137: y_pred 1.7995861848342953: loss: 0.0004138151657047118\n",
            "Iter #138: y_pred 1.7996119616263375: loss: 0.0003880383736625248\n",
            "Iter #139: y_pred 1.7996361330044324: loss: 0.0003638669955676743\n",
            "Iter #140: y_pred 1.7996587989272692: loss: 0.00034120107273083455\n",
            "Iter #141: y_pred 1.7996800531332222: loss: 0.0003199468667778316\n",
            "Iter #142: y_pred 1.799699983527016: loss: 0.00030001647298405487\n",
            "Iter #143: y_pred 1.7997186725424061: loss: 0.0002813274575939051\n",
            "Iter #144: y_pred 1.7997361974823547: loss: 0.0002638025176453507\n",
            "Iter #145: y_pred 1.7997526308380933: loss: 0.00024736916190670755\n",
            "Iter #146: y_pred 1.799768040588381: loss: 0.00023195941161913147\n",
            "Iter #147: y_pred 1.799782490480178: loss: 0.00021750951982202338\n",
            "Iter #148: y_pred 1.7997960402918904: loss: 0.0002039597081096023\n",
            "Iter #149: y_pred 1.799808746080261: loss: 0.00019125391973906503\n",
            "Iter #150: y_pred 1.799820660411923: loss: 0.00017933958807714312\n",
            "Iter #151: y_pred 1.799831832580562: loss: 0.0001681674194380367\n",
            "Iter #152: y_pred 1.7998423088105866: loss: 0.00015769118941344473\n",
            "Iter #153: y_pred 1.7998521324481318: loss: 0.00014786755186824152\n",
            "Iter #154: y_pred 1.799861344140195: loss: 0.00013865585980510353\n",
            "Iter #155: y_pred 1.7998699820026305: loss: 0.0001300179973695581\n",
            "Iter #156: y_pred 1.799878081777698: loss: 0.00012191822230200877\n",
            "Iter #157: y_pred 1.7998856769818123: loss: 0.00011432301818770085\n",
            "Iter #158: y_pred 1.799892799044106: loss: 0.00010720095589400458\n",
            "Iter #159: y_pred 1.7998994774363692: loss: 0.00010052256363080048\n",
            "Iter #160: y_pred 1.7999057397949083: loss: 9.426020509173405e-05\n",
            "Iter #161: y_pred 1.7999116120348224: loss: 8.838796517762759e-05\n",
            "Iter #162: y_pred 1.7999171184571694: loss: 8.288154283064841e-05\n",
            "Iter #163: y_pred 1.799922281849463: loss: 7.77181505371427e-05\n",
            "Iter #164: y_pred 1.799927123579917: loss: 7.287642008302342e-05\n",
            "Iter #165: y_pred 1.799931663685822: loss: 6.833631417801911e-05\n",
            "Iter #166: y_pred 1.7999359209564247: loss: 6.40790435753047e-05\n",
            "Iter #167: y_pred 1.7999399130106466: loss: 6.008698935344725e-05\n",
            "Iter #168: y_pred 1.7999436563699636: loss: 5.6343630036481684e-05\n",
            "Iter #169: y_pred 1.7999471665267535: loss: 5.283347324658294e-05\n",
            "Iter #170: y_pred 1.7999504580083818: loss: 4.954199161821826e-05\n",
            "Iter #171: y_pred 1.7999535444373065: loss: 4.6455562693559216e-05\n",
            "Iter #172: y_pred 1.7999564385874307: loss: 4.356141256933732e-05\n",
            "Iter #173: y_pred 1.7999591524369567: loss: 4.084756304334469e-05\n",
            "Iter #174: y_pred 1.7999616972179413: loss: 3.830278205874116e-05\n",
            "Iter #175: y_pred 1.7999640834627697: loss: 3.5916537230340495e-05\n",
            "Iter #176: y_pred 1.7999663210477341: loss: 3.367895226591422e-05\n",
            "Iter #177: y_pred 1.7999684192338996: loss: 3.1580766100436364e-05\n",
            "Iter #178: y_pred 1.7999703867054244: loss: 2.961329457562556e-05\n",
            "Iter #179: y_pred 1.7999722316054936: loss: 2.776839450646662e-05\n",
            "Iter #180: y_pred 1.7999739615700154: loss: 2.603842998460948e-05\n",
            "Iter #181: y_pred 1.7999755837592204: loss: 2.4416240779645548e-05\n",
            "Iter #182: y_pred 1.7999771048872897: loss: 2.289511271036382e-05\n",
            "Iter #183: y_pred 1.7999785312501404: loss: 2.146874985964331e-05\n",
            "Iter #184: y_pred 1.7999798687514796: loss: 2.0131248520405265e-05\n",
            "Iter #185: y_pred 1.7999811229272356: loss: 1.8877072764489355e-05\n",
            "Iter #186: y_pred 1.7999822989684693: loss: 1.770103153075908e-05\n",
            "Iter #187: y_pred 1.799983401742853: loss: 1.659825714694918e-05\n",
            "Iter #188: y_pred 1.7999844358148207: loss: 1.5564185179339773e-05\n",
            "Iter #189: y_pred 1.799985405464453: loss: 1.459453554697454e-05\n",
            "Iter #190: y_pred 1.7999863147051942: loss: 1.3685294805831916e-05\n",
            "Iter #191: y_pred 1.799987167300459: loss: 1.283269954099886e-05\n",
            "Iter #192: y_pred 1.799987966779212: loss: 1.2033220788021382e-05\n",
            "Iter #193: y_pred 1.7999887164505695: loss: 1.1283549430585182e-05\n",
            "Iter #194: y_pred 1.799989419417496: loss: 1.0580582504138292e-05\n",
            "Iter #195: y_pred 1.799990078589648: loss: 9.921410351942939e-06\n",
            "Iter #196: y_pred 1.7999906966954144: loss: 9.303304585595029e-06\n",
            "Iter #197: y_pred 1.7999912762932098: loss: 8.723706790281227e-06\n",
            "Iter #198: y_pred 1.799991819782063: loss: 8.180217937026057e-06\n",
            "Iter #199: y_pred 1.7999923294115467: loss: 7.670588453301264e-06\n",
            "Iter #200: y_pred 1.7999928072910878: loss: 7.192708912251433e-06\n",
            "Iter #201: y_pred 1.7999932553986981: loss: 6.744601301900133e-06\n",
            "Iter #202: y_pred 1.7999936755891608: loss: 6.3244108392535026e-06\n",
            "Iter #203: y_pred 1.799994069601709: loss: 5.93039829110964e-06\n",
            "Iter #204: y_pred 1.7999944390672216: loss: 5.560932778481131e-06\n",
            "Iter #205: y_pred 1.7999947855149756: loss: 5.214485024440663e-06\n",
            "Iter #206: y_pred 1.7999951103789755: loss: 4.88962102451751e-06\n",
            "Iter #207: y_pred 1.7999954150038868: loss: 4.584996113221607e-06\n",
            "Iter #208: y_pred 1.799995700650605: loss: 4.2993493949428085e-06\n",
            "Iter #209: y_pred 1.7999959685014708: loss: 4.031498529233346e-06\n",
            "Iter #210: y_pred 1.7999962196651662: loss: 3.7803348338361076e-06\n",
            "Iter #211: y_pred 1.7999964551813026: loss: 3.5448186974651463e-06\n",
            "Iter #212: y_pred 1.7999966760247241: loss: 3.323975275915103e-06\n",
            "Iter #213: y_pred 1.799996883109542: loss: 3.116890457954824e-06\n",
            "Iter #214: y_pred 1.799997077292919: loss: 2.9227070810211586e-06\n",
            "Iter #215: y_pred 1.7999972593786162: loss: 2.740621383834352e-06\n",
            "Iter #216: y_pred 1.7999974301203217: loss: 2.569879678393505e-06\n",
            "Iter #217: y_pred 1.7999975902247667: loss: 2.409775233358502e-06\n",
            "Iter #218: y_pred 1.799997740354656: loss: 2.2596453439494013e-06\n",
            "Iter #219: y_pred 1.7999978811314055: loss: 2.1188685945805474e-06\n",
            "Iter #220: y_pred 1.7999980131377178: loss: 1.9868622822549753e-06\n",
            "Iter #221: y_pred 1.7999981369199936: loss: 1.8630800064922681e-06\n",
            "Iter #222: y_pred 1.7999982529905918: loss: 1.7470094082483456e-06\n",
            "Iter #223: y_pred 1.799998361829952: loss: 1.6381700480572192e-06\n",
            "Iter #224: y_pred 1.7999984638885824: loss: 1.5361114176215551e-06\n",
            "Iter #225: y_pred 1.7999985595889243: loss: 1.4404110757482158e-06\n",
            "Iter #226: y_pred 1.7999986493271007: loss: 1.3506728993029071e-06\n",
            "Iter #227: y_pred 1.799998733474556: loss: 1.2665254440769047e-06\n",
            "Iter #228: y_pred 1.7999988123795947: loss: 1.187620405351808e-06\n",
            "Iter #229: y_pred 1.7999988863688199: loss: 1.1136311801607235e-06\n",
            "Iter #230: y_pred 1.7999989557484892: loss: 1.044251510817773e-06\n",
            "Iter #231: y_pred 1.7999990208057788: loss: 9.791942212622473e-07\n",
            "Iter #232: y_pred 1.7999990818099745: loss: 9.181900255672559e-07\n",
            "Iter #233: y_pred 1.7999991390135854: loss: 8.609864146080781e-07\n",
            "Iter #234: y_pred 1.7999991926533896: loss: 8.073466104541183e-07\n",
            "Iter #235: y_pred 1.799999242951413: loss: 7.570485871521981e-07\n",
            "Iter #236: y_pred 1.79999929011585: loss: 7.098841501296249e-07\n",
            "Iter #237: y_pred 1.799999334341924: loss: 6.656580759933917e-07\n",
            "Iter #238: y_pred 1.7999993758126962: loss: 6.241873038437262e-07\n",
            "Iter #239: y_pred 1.7999994146998228: loss: 5.8530017721381e-07\n",
            "Iter #240: y_pred 1.7999994511642656: loss: 5.488357344152206e-07\n",
            "Iter #241: y_pred 1.7999994853569594: loss: 5.146430406277602e-07\n",
            "Iter #242: y_pred 1.7999995174194343: loss: 4.825805657304727e-07\n",
            "Iter #243: y_pred 1.7999995474844044: loss: 4.5251559566139576e-07\n",
            "Iter #244: y_pred 1.7999995756763145: loss: 4.243236855216992e-07\n",
            "Iter #245: y_pred 1.7999996021118572: loss: 3.978881428778891e-07\n",
            "Iter #246: y_pred 1.799999626900455: loss: 3.7309954503683684e-07\n",
            "Iter #247: y_pred 1.7999996501447126: loss: 3.4985528740705263e-07\n",
            "Iter #248: y_pred 1.7999996719408438: loss: 3.280591562848656e-07\n",
            "Iter #249: y_pred 1.7999996923790667: loss: 3.0762093339298247e-07\n",
            "Iter #250: y_pred 1.79999971154398: loss: 2.884560199589714e-07\n",
            "Iter #251: y_pred 1.7999997295149115: loss: 2.704850885493215e-07\n",
            "Iter #252: y_pred 1.7999997463662463: loss: 2.536337537772937e-07\n",
            "Iter #253: y_pred 1.7999997621677362: loss: 2.378322638829644e-07\n",
            "Iter #254: y_pred 1.7999997769847864: loss: 2.2301521362955157e-07\n",
            "Iter #255: y_pred 1.7999997908787284: loss: 2.091212716326396e-07\n",
            "Iter #256: y_pred 1.7999998039070721: loss: 1.9609292789546373e-07\n",
            "Iter #257: y_pred 1.7999998161237445: loss: 1.8387625555504883e-07\n",
            "Iter #258: y_pred 1.7999998275793132: loss: 1.7242068683920309e-07\n",
            "Iter #259: y_pred 1.7999998383211948: loss: 1.6167880523276779e-07\n",
            "Iter #260: y_pred 1.7999998483938526: loss: 1.5160614741382972e-07\n",
            "Iter #261: y_pred 1.7999998578389798: loss: 1.421610202889667e-07\n",
            "Iter #262: y_pred 1.7999998666956716: loss: 1.3330432846458962e-07\n",
            "Iter #263: y_pred 1.7999998750005881: loss: 1.249994119323361e-07\n",
            "Iter #264: y_pred 1.7999998827881043: loss: 1.1721189574487312e-07\n",
            "Iter #265: y_pred 1.7999998900904555: loss: 1.0990954457668067e-07\n",
            "Iter #266: y_pred 1.7999998969378668: loss: 1.0306213327204716e-07\n",
            "Iter #267: y_pred 1.7999999033586815: loss: 9.664131850328772e-08\n",
            "Iter #268: y_pred 1.7999999093794767: loss: 9.062052330754966e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR5WBYto4NV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "c049d395-755e-4e96-f84f-2e5c940f7e54"
      },
      "source": [
        "loss = pd.DataFrame({'train_loss': model['train_loss']})\n",
        "# robi kolumnę \"iter\"\n",
        "loss = loss.reset_index().rename(columns={'index': 'iter'})\n",
        "# zwiększa każdą liczbę z \"iter\" o 1\n",
        "loss['iter'] += 1\n",
        "loss.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.322249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.279680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.237540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.195837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.154590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iter  train_loss\n",
              "0     1    1.322249\n",
              "1     2    1.279680\n",
              "2     3    1.237540\n",
              "3     4    1.195837\n",
              "4     5    1.154590"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UV2RRs_5H7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "199a7940-4430-4213-d5a8-8f131a86c041"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=loss['iter'], y=loss['train_loss'], mode='markers+lines'))\n",
        "fig.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"31829b50-645e-4b83-b3fd-24021d49faff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"31829b50-645e-4b83-b3fd-24021d49faff\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '31829b50-645e-4b83-b3fd-24021d49faff',\n",
              "                        [{\"mode\": \"markers+lines\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269], \"y\": [1.3222491758193113, 1.2796802835004126, 1.2375396912201282, 1.195836910115009, 1.1545903528778616, 1.113825733183146, 1.0735745588263415, 1.0338727210671395, 0.9947591840763658, 0.9562747789254677, 0.9184611063799996, 0.8813595520160398, 0.8450104160166874, 0.8094521585625345, 0.7747207601317849, 0.7408491943849318, 0.7078670097213877, 0.6758000141389238, 0.6446700567600165, 0.6144948983526191, 0.5852881623892925, 0.5570593576652945, 0.5298139632270482, 0.5035535663304791, 0.47827604432913984, 0.4539757817544767, 0.4306439143616636, 0.4082685925399354, 0.38683525719291034, 0.3663269219508194, 0.34672445635499227, 0.3280068654311443, 0.31015156182202497, 0.29313462736591367, 0.27693106167339354, 0.26151501586262293, 0.2468600101579752, 0.23293913453630077, 0.21972523201933258, 0.20719106456190017, 0.19530946177702546, 0.1840534529750013, 0.17339638317921757, 0.16331201392216022, 0.15377460972605572, 0.14475901123953738, 0.13624069603946243, 0.12819582812054886, 0.12060129708913325, 0.11343474805508214, 0.10667460318125066, 0.10030007580592404, 0.09429117800304665, 0.08862872239001085, 0.08329431893515449, 0.07827036745851967, 0.07354004646100076, 0.06908729885980125, 0.06489681515284218, 0.06095401448196047, 0.057245024014835444, 0.05375665701877552, 0.050476389955990575, 0.04739233888978056, 0.04449323545417161, 0.04176840260586756, 0.039207730346837666, 0.036801651578276307, 0.034541118221875555, 0.03241757772221532, 0.030422950024326934, 0.028549605103036946, 0.026790341105274562, 0.025138363153021093, 0.023587262842768464, 0.022130998467092544, 0.02076387597510143, 0.019480530680898278, 0.018275909722715422, 0.017145255269879645, 0.016084088470153635, 0.015088194126174459, 0.014153606086547033, 0.013276593334619502, 0.012453646755933745, 0.011681466563781662, 0.01095695036113331, 0.010277181816358505, 0.009639419929637905, 0.009041088866660107, 0.008479768336125959, 0.007953184487675458, 0.0074592013070977625, 0.0069958124860516335, 0.006561133743990144, 0.006153395580521748, 0.005770936437060392, 0.00541219624725553, 0.00507571035640364, 0.0047601037907423205, 0.00446408585826763, 0.004186445063450783, 0.003926044318976585, 0.003681816438350527, 0.003452759893960433, 0.0032379348258841922, 0.0030364592874383423, 0.002847505714140164, 0.002670297603420302, 0.002504106393061978, 0.00234824852695259, 0.002202082697345231, 0.00206500725337877, 0.0019364577661715732, 0.001815904741311547, 0.0017028514700814235, 0.0015968320112282886, 0.0014974092955390983, 0.0014041733459302375, 0.0013167396061577463, 0.0012347473716576296, 0.001157858316390925, 0.0010857551099230367, 0.0010181401193039008, 0.000954734190625306, 0.000895275505438553, 0.0008395185074945299, 0.0007872328955313979, 0.0007382026781050932, 0.0006922252866652379, 0.0006491107433419518, 0.0006086808800898069, 0.0005707686060514305, 0.0005352172201886773, 0.000501879766408031, 0.0004706184285683257, 0.0004413039629240778, 0.0004138151657047118, 0.0003880383736625248, 0.0003638669955676743, 0.00034120107273083455, 0.0003199468667778316, 0.00030001647298405487, 0.0002813274575939051, 0.0002638025176453507, 0.00024736916190670755, 0.00023195941161913147, 0.00021750951982202338, 0.0002039597081096023, 0.00019125391973906503, 0.00017933958807714312, 0.0001681674194380367, 0.00015769118941344473, 0.00014786755186824152, 0.00013865585980510353, 0.0001300179973695581, 0.00012191822230200877, 0.00011432301818770085, 0.00010720095589400458, 0.00010052256363080048, 9.426020509173405e-05, 8.838796517762759e-05, 8.288154283064841e-05, 7.77181505371427e-05, 7.287642008302342e-05, 6.833631417801911e-05, 6.40790435753047e-05, 6.008698935344725e-05, 5.6343630036481684e-05, 5.283347324658294e-05, 4.954199161821826e-05, 4.6455562693559216e-05, 4.356141256933732e-05, 4.084756304334469e-05, 3.830278205874116e-05, 3.5916537230340495e-05, 3.367895226591422e-05, 3.1580766100436364e-05, 2.961329457562556e-05, 2.776839450646662e-05, 2.603842998460948e-05, 2.4416240779645548e-05, 2.289511271036382e-05, 2.146874985964331e-05, 2.0131248520405265e-05, 1.8877072764489355e-05, 1.770103153075908e-05, 1.659825714694918e-05, 1.5564185179339773e-05, 1.459453554697454e-05, 1.3685294805831916e-05, 1.283269954099886e-05, 1.2033220788021382e-05, 1.1283549430585182e-05, 1.0580582504138292e-05, 9.921410351942939e-06, 9.303304585595029e-06, 8.723706790281227e-06, 8.180217937026057e-06, 7.670588453301264e-06, 7.192708912251433e-06, 6.744601301900133e-06, 6.3244108392535026e-06, 5.93039829110964e-06, 5.560932778481131e-06, 5.214485024440663e-06, 4.88962102451751e-06, 4.584996113221607e-06, 4.2993493949428085e-06, 4.031498529233346e-06, 3.7803348338361076e-06, 3.5448186974651463e-06, 3.323975275915103e-06, 3.116890457954824e-06, 2.9227070810211586e-06, 2.740621383834352e-06, 2.569879678393505e-06, 2.409775233358502e-06, 2.2596453439494013e-06, 2.1188685945805474e-06, 1.9868622822549753e-06, 1.8630800064922681e-06, 1.7470094082483456e-06, 1.6381700480572192e-06, 1.5361114176215551e-06, 1.4404110757482158e-06, 1.3506728993029071e-06, 1.2665254440769047e-06, 1.187620405351808e-06, 1.1136311801607235e-06, 1.044251510817773e-06, 9.791942212622473e-07, 9.181900255672559e-07, 8.609864146080781e-07, 8.073466104541183e-07, 7.570485871521981e-07, 7.098841501296249e-07, 6.656580759933917e-07, 6.241873038437262e-07, 5.8530017721381e-07, 5.488357344152206e-07, 5.146430406277602e-07, 4.825805657304727e-07, 4.5251559566139576e-07, 4.243236855216992e-07, 3.978881428778891e-07, 3.7309954503683684e-07, 3.4985528740705263e-07, 3.280591562848656e-07, 3.0762093339298247e-07, 2.884560199589714e-07, 2.704850885493215e-07, 2.536337537772937e-07, 2.378322638829644e-07, 2.2301521362955157e-07, 2.091212716326396e-07, 1.9609292789546373e-07, 1.8387625555504883e-07, 1.7242068683920309e-07, 1.6167880523276779e-07, 1.5160614741382972e-07, 1.421610202889667e-07, 1.3330432846458962e-07, 1.249994119323361e-07, 1.1721189574487312e-07, 1.0990954457668067e-07, 1.0306213327204716e-07, 9.664131850328772e-08, 9.062052330754966e-08]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('31829b50-645e-4b83-b3fd-24021d49faff');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s66fHa43LDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc297a5-b7ae-4a38-9175-52e6d0e3d793"
      },
      "source": [
        "predict(X, model['W1'], model['W2'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7999999093794767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}