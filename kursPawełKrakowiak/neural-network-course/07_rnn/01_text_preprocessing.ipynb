{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KamilBienias/data-science/blob/main/kursPawe%C5%82Krakowiak/neural-network-course/07_rnn/01_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_X7jpgqlRyb"
      },
      "source": [
        "### Preprocessing danych tekstowch - wektoryzacja tekstu\n",
        "\n",
        "1. [Podział tekstu na słowa](#a0)\n",
        "2. [Kodowanie *one_hot()*](#a1)\n",
        "3. [Kodowanie *hashing_trick()*](#a2)\n",
        "4. [Tokenizer](#a3)\n",
        "\n",
        "Nie możemy wprowadzić bezpośrednio danych tekstowych do sieci neuronowej! Musimy te dane odpowiednio przygotować (preprocessing). Dane tekstowe muszą zostać zakodowane za pomocą liczb aby mogły być wprowadzone do sieci neuronowej. \n",
        "\n",
        "Biblioteka Keras zawiera kilka narzędzi, które możemy wykorzystać do przygotowania naszych danych. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vlI9iE3pLNa"
      },
      "source": [
        "### <a name='a0'></a> Podział tekstu na słowa\n",
        "\n",
        "Często w NLP - Natural Language Processing używamy słowa token. Tokenem może być pojedyńczy znak, a także cały wyraz. Wsystko zależy od kontekstu i potrzeb. Apy podzielić tekst na tokeny (w tym przypadku wyrazy) użyjemy funkcji *text_to_word_sequence()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me1IMXXHpIgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f155a24e-2d66-4942-b4c8-0879b2d8dd4e"
      },
      "source": [
        "# ######################################################\n",
        "# Etap 11. Odcinek: Praca z tekstem - wektoryzacja.\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = 'Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.'\n",
        "\n",
        "# zamienia duże na małe litery, pomija znaki interpunkcyjne\n",
        "tokens = text_to_word_sequence(text)\n",
        "tokens"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keras',\n",
              " 'is',\n",
              " 'a',\n",
              " 'high',\n",
              " 'level',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'api',\n",
              " 'written',\n",
              " 'in',\n",
              " 'python',\n",
              " 'and',\n",
              " 'capable',\n",
              " 'of',\n",
              " 'running',\n",
              " 'on',\n",
              " 'top',\n",
              " 'of',\n",
              " 'tensorflow',\n",
              " 'cntk',\n",
              " 'or',\n",
              " 'theano']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73WDqyIqchw"
      },
      "source": [
        "### <a name='a1'></a> Kodowanie *one_hot()*\n",
        "\n",
        "Nazwa sugeruje, że tworzymy kodowanie zero-jednykowe dokumentu, co nie jest prawdą. Polega na przedstawieniu każdego słowa jako unikalnej liczby całkowitej. *one_hot(text, n)* koduje tekst do listy indeksów słów o rozmiarze n. Jest to opakowanie funkcji hashing_trick używającej hash jako funkcji hashującej. \n",
        "\n",
        "Jednoznaczność mapowania słów na indeksy nie jest gwarantowana.  Zastosowanie funkcji hashującej może powodować kolizje i nie wszystkim słowom zostaną przypisane unikalne wartości całkowite.\n",
        "\n",
        "Oprócz tekstu należy podać rozmiar słownika. Może to być łączna liczba słów w dokumencie lub więcej, jeśli zamierzamy zakodować dodatkowe dokumenty zawierające dodatkowe słowa.\n",
        "\n",
        " Rozmiar słownika określa przestrzeń hashująca, z której słowa są hashowane. Najlepiej byłoby, gdyby był on większy niż słownik o pewien procent (np. 25%), aby zminimalizować liczbę kolizji. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BVXixRQGH8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4feed48d-b49a-4a3d-a240-e8c5bde37b13"
      },
      "source": [
        "hash('sieć')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4252116340759207180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWeGs1X3Gf4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b704a1-d993-4367-a525-2314cebb8758"
      },
      "source": [
        "# dzielenie modulo 100 czyli reszta z dzielenia przez 100.\r\n",
        "# Ta liczba może stanowić indeks naszego słowa\r\n",
        "hash('sieć') % 100"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BcpUUiGPXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a371ec64-be6c-40b6-cd9e-021f7239b056"
      },
      "source": [
        "hash('neuronowa')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "994319659138035343"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuzo2jNVFZj-",
        "outputId": "3718960f-2a3d-4b90-b47e-5fa10a76d7df"
      },
      "source": [
        "hash('neuronowa') % 100"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TBwBWt4GTJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0b4b8d-07cf-4c89-a2aa-31f6ead7532e"
      },
      "source": [
        "# hash zawsze zwraca tą samą wartość dla tego samego elementu\r\n",
        "hash('sieć')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4252116340759207180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd_I9uaprTty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf382ac-f3ba-4ed2-b302-85902ae38767"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "# weźmy unikalne słowa z listy tokenów (miała 22 elementy) do zbioru words\n",
        "words = set(tokens)\n",
        "print(words)\n",
        "print(type(words))\n",
        "# zwiększa rozmiar o 30% w stosunku do unikalnych słów\n",
        "one_hot_tokens = one_hot(text, round(len(words) * 1.3))\n",
        "print(one_hot_tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'keras', 'tensorflow', 'running', 'and', 'python', 'written', 'top', 'networks', 'level', 'neural', 'on', 'of', 'or', 'a', 'theano', 'in', 'api', 'capable', 'cntk', 'high', 'is'}\n",
            "<class 'set'>\n",
            "[23, 20, 23, 10, 3, 19, 8, 15, 7, 26, 24, 16, 15, 3, 10, 9, 17, 3, 26, 19, 19, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Hqu8FJuzeS"
      },
      "source": [
        "### <a name='a2'></a> Kodowanie *hashing_trick()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5E8P4w2u3Zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33b5b43-e2a1-43f4-9c7c-4a4cf77df159"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "\n",
        "hashing_trick(text, round(len(words) * 1.3), hash_function='md5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7,\n",
              " 20,\n",
              " 22,\n",
              " 24,\n",
              " 20,\n",
              " 14,\n",
              " 19,\n",
              " 11,\n",
              " 10,\n",
              " 4,\n",
              " 4,\n",
              " 15,\n",
              " 19,\n",
              " 15,\n",
              " 18,\n",
              " 23,\n",
              " 14,\n",
              " 15,\n",
              " 5,\n",
              " 20,\n",
              " 2,\n",
              " 8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfMd5M9TlwPy"
      },
      "source": [
        "### <a name='a3'></a> Tokenizer\n",
        "Keras dostarcza klasę Tokenizer do przygotowywania dokumentów tekstowych do uczenia głębokiego. \n",
        "\n",
        "Klasa *Tokenizer* pozwala zamienić każdy tekst na sekwencję liczb całkowitych w taki sposób, że każda liczba całkowita jest indeksem tokenu w słowniku. Tokenem zazwyczaj jest pojedyncze słowo. Zero jest zarezerwowanym ideksem i nie może zostać przypisane do żadnego słowa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4HmQB8elcfI"
      },
      "source": [
        "# ######################################################\n",
        "# Etap 11. Odcinek: Praca z tekstem - tokenizacja.\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# num_words - zachowana zostanie określona liczba słów ze względu na częstotliwość\n",
        "tokenizer = Tokenizer()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9RmalTmmrxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0061975e-63ca-4ec3-aa07-63a69765cc42"
      },
      "source": [
        "# dokumnenty, którymi jest przykład zbiór komentarzy pod zdjęciami na fb\n",
        "samples = ['Great picture!', 'Nice view', 'Good to see you :)', 'Good picture!', 'Good']\n",
        "\n",
        "# dopasowanie tokenizera na tych dokumentach\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "tokenizer.index_word"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'good',\n",
              " 2: 'picture',\n",
              " 3: 'great',\n",
              " 4: 'nice',\n",
              " 5: 'view',\n",
              " 6: 'to',\n",
              " 7: 'see',\n",
              " 8: 'you'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4juXUFP0nHJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b7d255-b46e-40e5-e835-1f0b0bd4e977"
      },
      "source": [
        "# uporządkowany słownik pokazujący liczebność słowa w naszych próbkach\r\n",
        "tokenizer.word_counts"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('great', 1),\n",
              "             ('picture', 2),\n",
              "             ('nice', 1),\n",
              "             ('view', 1),\n",
              "             ('good', 3),\n",
              "             ('to', 1),\n",
              "             ('see', 1),\n",
              "             ('you', 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-DWVcdeoBj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a62a9b3-1210-4b30-e2fb-63a1e7173147"
      },
      "source": [
        "# liczy ilość wszystkich dokumentów\r\n",
        "tokenizer.document_count"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n0LPVKZwNxo"
      },
      "source": [
        "Po dopasowaniu Tokenizera do danych treningowych można go użyć do kodowania dokumentów danych treningowych jak i danych testowych.\n",
        "\n",
        "Funkcja *texts_to_matrix()* tworzy wektor dla każdego dokumentu. Długość wektora jest równa długości unikalnych słów we wszystkich dokumentach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcQY6xIVyNSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6403a39d-1066-4cd5-9ca2-782b7e986001"
      },
      "source": [
        "print(tokenizer.index_word)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'good', 2: 'picture', 3: 'great', 4: 'nice', 5: 'view', 6: 'to', 7: 'see', 8: 'you'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoHbCSCvoQu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc05ad85-ff3b-4ce9-99b7-dd1ab7d0dd3b"
      },
      "source": [
        "# macierz składająca się z wektorów długości. Każdy wektor długości, którą jest ilość\r\n",
        "# unikalnych słów we wszystkich dokumentach. Wartości w pierwszej kolumnie tej macierzy\r\n",
        "# zawsze będą zerami bo zero nie może być zarezerwowane do żadnego słowa. \r\n",
        "# Druga kolumna to słowo 'good' bo ono najczęściej się pojawiało (w recenzjach trzeciej, czwartej i piątej\r\n",
        "# dlatego tam są jedynki). \r\n",
        "# Trzecia kolumna to słowo 'picture', które było w dokumentach pierwszym i czwartym\r\n",
        "tokenizer.texts_to_matrix(samples)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ko2M6i1K55C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}